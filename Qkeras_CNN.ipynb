{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Qkeras CNN BN &PO2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1miUoB4oiUak"
      },
      "source": [
        "#Importing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtKs8iTj53r4"
      },
      "source": [
        "#Tensorflow version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkofaVEB30oN"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_KQZKwjzI2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063ee229-6a52-4ecd-d287-a59b1b1c3ebf"
      },
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_model_optimization in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.6)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_aQv0xtgWMn",
        "outputId": "5ca8b7c8-fc39-4e5e-8177-73706e1a81e9"
      },
      "source": [
        "!git clone https://github.com/google/qkeras.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'qkeras' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS7h-rFphEFl",
        "outputId": "7e7399fb-5aae-4eb1-fe37-7ee46b36c526"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/UCI HAR Dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-28 03:30:10--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60999314 (58M) [application/x-httpd-php]\n",
            "Saving to: ‘UCI HAR Dataset.zip.1’\n",
            "\n",
            "UCI HAR Dataset.zip 100%[===================>]  58.17M  77.0MB/s    in 0.8s    \n",
            "\n",
            "2021-04-28 03:30:11 (77.0 MB/s) - ‘UCI HAR Dataset.zip.1’ saved [60999314/60999314]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzNd__e7zBU8"
      },
      "source": [
        "import sys\n",
        "sys.path.append('qkeras')\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from qkeras import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_Hq0VUMvJx9"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from scipy import ndimage\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D,Conv2D\n",
        "from keras.layers.convolutional import MaxPooling1D,MaxPooling2D\n",
        "from keras.layers import BatchNormalization,ReLU,GlobalAveragePooling1D,MaxPooling1D,LSTM,TimeDistributed,GlobalAveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import save_model, load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import  plot_model\n",
        "from keras.models import Model,save_model,load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import concatenate\n",
        "import numpy as np\n",
        "from six.moves import zip\n",
        "from tensorflow.keras import callbacks\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from qkeras.utils import load_qmodel\n",
        "from qkeras.estimate import print_qstats\n",
        "from datetime import datetime\n",
        "\n",
        "from packaging import version\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import expand_dims\n",
        "from tensorflow import keras\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8tRrh2Q2gnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d6e9c1-fc5a-4fea-a943-a40c2dffff26"
      },
      "source": [
        "!pip install -U tensorboard_plugin_profile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboard_plugin_profile in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: gviz-api>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INpMNpCdimZr"
      },
      "source": [
        "#Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vYcHaInEDoK"
      },
      "source": [
        "def load_file(filepath):\n",
        "  dataframe=read_csv(filepath,header=None,delim_whitespace=True)\n",
        "  return dataframe.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7akEplVIDyX"
      },
      "source": [
        "def load_group(filenames,prefix=''):\n",
        "  loaded=list()\n",
        "  for name in filenames:\n",
        "    data=load_file(prefix+name)\n",
        "    loaded.append(data)\n",
        "  loaded=dstack(loaded)\n",
        "  return loaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKHYErTsIYac"
      },
      "source": [
        "def load_dataset_group(group,prefix=''):\n",
        "  filepath=prefix+group+'/Inertial Signals/'\n",
        "  filenames=list()\n",
        "  filenames+=['total_acc_x_'+group+'.txt','total_acc_y_'+group+'.txt','total_acc_z_'+group+'.txt']\n",
        "  filenames+=['body_acc_x_'+group+'.txt','body_acc_y_'+group+'.txt','body_acc_z_'+group+'.txt']\n",
        "  filenames+=['body_gyro_x_'+group+'.txt','body_gyro_y_'+group+'.txt','body_gyro_z_'+group+'.txt']\n",
        "  X=load_group(filenames,filepath)\n",
        "  y=load_file(prefix+group+'/y_'+group+'.txt')\n",
        "  return (X,y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuWfdQHuJt9f"
      },
      "source": [
        "def load_dataset(prefix=''):\n",
        "  trainX,trainy=load_dataset_group('train',prefix+'UCI HAR Dataset/')\n",
        "  print(trainX.shape,trainy.shape)\n",
        "  testX,testy=load_dataset_group('test',prefix+'UCI HAR Dataset/')\n",
        "  print(testX.shape,testy.shape)\n",
        "  trainy=trainy-1\n",
        "  testy=testy-1\n",
        "  trainy=to_categorical(trainy)\n",
        "  testy=to_categorical(testy)\n",
        "  print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "  return trainX, trainy, testX, testy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Mtbf_PSH87"
      },
      "source": [
        "def scale_data(trainX, testX):\n",
        "\t# remove overlap\n",
        "\tcut = int(trainX.shape[1] / 2)\n",
        "\tlongX = trainX[:, -cut:, :]\n",
        "\t# flatten windows\n",
        "\tlongX = longX.reshape((longX.shape[0] * longX.shape[1], longX.shape[2]))\n",
        "\t# flatten train and test\n",
        "\tflatTrainX = trainX.reshape((trainX.shape[0] * trainX.shape[1], trainX.shape[2]))\n",
        "\tflatTestX = testX.reshape((testX.shape[0] * testX.shape[1], testX.shape[2]))\n",
        "\t# standardize\n",
        "\n",
        "\ts = StandardScaler()\n",
        "\t\t# fit on training data\n",
        "\ts.fit(longX)\n",
        "\t\t# apply to training and test data\n",
        "\tlongX = s.transform(longX)\n",
        "\tflatTrainX = s.transform(flatTrainX)\n",
        "\tflatTestX = s.transform(flatTestX)\n",
        "\t# reshape\n",
        "\tflatTrainX = flatTrainX.reshape((trainX.shape))\n",
        "\tflatTestX = flatTestX.reshape((testX.shape))\n",
        "\treturn flatTrainX, flatTestX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dow8qJO4myQ"
      },
      "source": [
        "##Function for model size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytB8g1p74mEU"
      },
      "source": [
        "def get_zipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  zipped_file = file+'.zip'\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JyEh2LQyggX"
      },
      "source": [
        "optimizer=Adam(lr=0.0001)\n",
        "with_bn=1\n",
        "THRESHOLD=0.1\n",
        "\n",
        "class LearningRateAdjuster(callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    self.learning_rate_factor = 1.0\n",
        "    pass\n",
        "\n",
        "  def on_epoch_end(self, epochs, logs):\n",
        "    max_variance = -1\n",
        "\n",
        "    for layer in self.model.layers:\n",
        "      if layer.__class__.__name__ in [\n",
        "          \"BatchNormalization\",\n",
        "          \"QBatchNormalization\"\n",
        "      ]:\n",
        "        variance = np.max(layer.get_weights()[-1])\n",
        "        if variance > max_variance:\n",
        "          max_variance = variance\n",
        "\n",
        "    if max_variance > 32 and self.learning_rate_factor < 100:\n",
        "      learning_rate = K.get_value(self.model.optimizer.learning_rate)\n",
        "      self.learning_rate_factor /= 2.0\n",
        "      print(\"***** max_variance is {} / lr is {} *****\".format(\n",
        "          max_variance, learning_rate))\n",
        "      K.eval(K.update(\n",
        "          self.model.optimizer.learning_rate, learning_rate / 2.0\n",
        "      ))\n",
        "\n",
        "lra = LearningRateAdjuster()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOWuN0ZroUDR"
      },
      "source": [
        "##1. CNN MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7BkQ1pYraEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9b31fc0-6eda-4a8b-e2d9-b467d249c575"
      },
      "source": [
        "trainX,trainy,testX,testy=load_dataset()\n",
        "verbose,epochs,batch_size=1,500,32 #500\n",
        "n_timesteps,n_features,n_outputs=trainX.shape[1],trainX.shape[2],trainy.shape[1]\n",
        "print('n step: ', n_timesteps)\n",
        "print('features: ',n_features)\n",
        "trainX,testX=scale_data(trainX,testX)\n",
        "print(trainX.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            "n step:  128\n",
            "features:  9\n",
            "(7352, 128, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yciMN9WVMkpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00437f8b-2dc1-4737-a09a-f469562d6741"
      },
      "source": [
        "\n",
        "'''\n",
        "\n",
        "inputs=keras.Input(shape=(n_timesteps,n_features))\n",
        "\n",
        "\n",
        "conv_1=tf.keras.layers.Conv1D(filters=64,kernel_size=5,strides=2,activation='relu')(inputs)\n",
        "maxpool_1=tf.keras.layers.MaxPooling1D(pool_size=2,strides=2)(conv_1)\n",
        "\n",
        "conv_2=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu')(maxpool_1)\n",
        "maxpool_2=tf.keras.layers.MaxPooling1D(pool_size=2,strides=1)(conv_2)\n",
        "\n",
        "conv_3=tf.keras.layers.Conv1D(filters=32,kernel_size=3,strides=1,activation='relu')(maxpool_2)\n",
        "avg_pooling=tf.keras.layers.GlobalAveragePooling1D()(conv_3)\n",
        "batch_norm=tf.keras.layers.BatchNormalization()(avg_pooling)\n",
        "\n",
        "output=tf.keras.layers.Dense(n_outputs,activation='softmax')(batch_norm)\n",
        "model=tf.keras.Model(inputs=inputs,outputs=output)\n",
        "'''\n",
        "\n",
        "x=x_in=Input(shape=(n_timesteps,n_features),name='input')\n",
        "x=QActivation(\"quantized_relu_po2(4,1)\",name=\"acti\")(x)\n",
        "x=QConv1D(filters=64, kernel_size=3,\n",
        "    strides=1,\n",
        "    kernel_quantizer=quantized_po2(4, 1),\n",
        "    bias_quantizer=quantized_bits(4,1) ,\n",
        "    name=\"conv1d_0_m\")(x)\n",
        "x = QActivation(\"quantized_relu(3,1)\", name=\"act0_m\")(x)\n",
        "x=tf.keras.layers.MaxPooling1D(pool_size=2,strides=2)(x)\n",
        "x=QConv1D(filters=128, kernel_size=3,\n",
        "    strides=1,\n",
        "    kernel_quantizer=quantized_po2(4, 1),\n",
        "    bias_quantizer=quantized_bits(4,1),\n",
        "    name=\"conv1d_1_m\")(x)\n",
        "x = QActivation(\"quantized_relu(3,1)\", name=\"act1_m\")(x)\n",
        "x=tf.keras.layers.MaxPooling1D(pool_size=2,strides=2)(x)\n",
        "x=QConv1D(filters=32, kernel_size=3,\n",
        "    strides=1,\n",
        "    kernel_quantizer=quantized_po2(4, 1),\n",
        "    bias_quantizer=quantized_bits(4,2,2),\n",
        "    name=\"conv1d_2_m\")(x)\n",
        "x = QActivation(\"quantized_relu(3,1)\", name=\"act2_m\")(x)\n",
        "x=tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "if with_bn:\n",
        "  x=QBatchNormalization(\n",
        "      gamma_quantizer=quantized_relu_po2(4,8),\n",
        "      variance_quantizer=quantized_relu_po2(6),\n",
        "      beta_quantizer=quantized_po2(4, 4),\n",
        "      gamma_range=8,\n",
        "      beta_range=4,\n",
        "      name=\"bn1\")(x)\n",
        "x = QActivation(\"quantized_relu(3,1)\", name=\"act3_m\")(x)\n",
        "x = Flatten()(x)\n",
        "x = QDense(\n",
        "    n_outputs,\n",
        "    kernel_quantizer=quantized_ulaw(4, 0, 1),\n",
        "    bias_quantizer=quantized_bits(4, 0, 1),\n",
        "    name=\"dense\")(\n",
        "        x)\n",
        "x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "model = Model(inputs=[x_in], outputs=[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "qkeras/qkeras/qnormalization.py:81: UserWarning: gamma_range is deprecated in QBatchNormalization layer.\n",
            "  warnings.warn('gamma_range is deprecated in QBatchNormalization layer.')\n",
            "qkeras/qkeras/qnormalization.py:84: UserWarning: beta_range is deprecated in QBatchNormalization layer.\n",
            "  warnings.warn('beta_range is deprecated in QBatchNormalization layer.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEENq8DHseVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33047a9c-1aa8-4cf7-8fcd-8db8328d57a9"
      },
      "source": [
        "plot_model(model, show_shapes=True, to_file='CNN_Model.png')\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "      trainX, trainy, batch_size=batch_size,\n",
        "      epochs=epochs, initial_epoch=1, verbose=1,\n",
        "      validation_split=0.1,\n",
        "      callbacks=[]) #lra])\n",
        "\n",
        "outputs = []\n",
        "output_names = []\n",
        "\n",
        "for layer in model.layers:\n",
        "  if layer.__class__.__name__ in [\n",
        "        \"QActivation\", \"QBatchNormalization\", \"Activation\", \"QDense\",\n",
        "        \"QConv1D\", \"QDepthwiseConv2D\"\n",
        "  ]:\n",
        "    output_names.append(layer.name)\n",
        "    outputs.append(layer.output)\n",
        "\n",
        "model_debug = Model(inputs=[x_in], outputs=outputs)\n",
        "\n",
        "outputs = model_debug.predict(trainX)\n",
        "\n",
        "print(\"{:30} {: 8.4f} {: 8.4f}\".format(\n",
        "      \"input\", np.min(trainX), np.max(trainX)))\n",
        "\n",
        "for n, p in zip(output_names, outputs):\n",
        "  print(\"{:30} {: 8.4f} {: 8.4f}\".format(n, np.min(p), np.max(p)), end=\"\")\n",
        "  layer = model.get_layer(n)\n",
        "  for i, weights in enumerate(layer.get_weights()):\n",
        "    if layer.get_quantizers()[i]:\n",
        "      weights = K.eval(layer.get_quantizers()[i](K.constant(weights)))\n",
        "    print(\" ({: 8.4f} {: 8.4f})\".format(np.min(weights), np.max(weights)),\n",
        "            end=\"\")\n",
        "  print(\"\")\n",
        "\n",
        "score = model.evaluate(testX, testy, verbose=False)\n",
        "print(\"Test score:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "FILE='/content/drive/My Drive/Colab/MS Project/qkeras-master/qkeras-master/My Qkeras Model/model.h5'\n",
        "model.save(FILE)\n",
        "print_qstats(model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/5\n",
            "207/207 [==============================] - 8s 14ms/step - loss: 1.5339 - accuracy: 0.3957 - val_loss: 1.1467 - val_accuracy: 0.3519\n",
            "Epoch 3/5\n",
            "207/207 [==============================] - 2s 7ms/step - loss: 0.7947 - accuracy: 0.6728 - val_loss: 0.6867 - val_accuracy: 0.7120\n",
            "Epoch 4/5\n",
            "207/207 [==============================] - 2s 7ms/step - loss: 0.6700 - accuracy: 0.7378 - val_loss: 0.3354 - val_accuracy: 0.9579\n",
            "Epoch 5/5\n",
            "207/207 [==============================] - 2s 7ms/step - loss: 0.4799 - accuracy: 0.8651 - val_loss: 0.2760 - val_accuracy: 0.9660\n",
            "input                          -15.7022  15.1091\n",
            "acti                             0.0000   1.0000\n",
            "conv1d_0_m                      -4.1172   3.6074 ( -0.5000   0.5000) (  0.0000   0.0000)\n",
            "act0_m                           0.0000   1.7500\n",
            "conv1d_1_m                      -4.2178   4.5254 ( -0.2500   0.2500) (  0.0000   0.0000)\n",
            "act1_m                           0.0000   1.7500\n",
            "conv1d_2_m                      -3.4941   4.5635 ( -0.2500   0.2500) (  0.0000   0.0000)\n",
            "act2_m                           0.0000   1.7500\n",
            "bn1                             -4.0309   8.5392 (  1.0000   1.0000) ( -0.0625   0.0625) (  0.0039   1.0000) (  0.0002   0.5000)\n",
            "act3_m                           0.0000   1.7500\n",
            "dense                           -7.8012   8.2731 ( -0.8750   0.8750) (  0.0000   0.1250)\n",
            "softmax                          0.0000   0.9988\n",
            "Test score: 0.4915546178817749\n",
            "Test accuracy: 0.873430609703064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "qkeras/qkeras/qnormalization.py:81: UserWarning: gamma_range is deprecated in QBatchNormalization layer.\n",
            "  warnings.warn('gamma_range is deprecated in QBatchNormalization layer.')\n",
            "qkeras/qkeras/qnormalization.py:84: UserWarning: beta_range is deprecated in QBatchNormalization layer.\n",
            "  warnings.warn('beta_range is deprecated in QBatchNormalization layer.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From qkeras/qkeras/estimate.py:345: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From qkeras/qkeras/estimate.py:345: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of operations in model:\n",
            "    conv1d_0_m                    : 217728 (sadder_4_4)\n",
            "    conv1d_1_m                    : 1499136 (sbarrel_4_3)\n",
            "    conv1d_2_m                    : 344064 (sbarrel_4_3)\n",
            "    dense                         : 192   (smult_4_3)\n",
            "\n",
            "Number of operation types in model:\n",
            "    sadder_4_4                    : 217728\n",
            "    sbarrel_4_3                   : 1843200\n",
            "    smult_4_3                     : 192\n",
            "\n",
            "Weight profiling:\n",
            "    conv1d_0_m_weights             : 1728  (4-bit unit)\n",
            "    conv1d_0_m_bias                : 64    (4-bit unit)\n",
            "    conv1d_1_m_weights             : 24576 (4-bit unit)\n",
            "    conv1d_1_m_bias                : 128   (4-bit unit)\n",
            "    conv1d_2_m_weights             : 12288 (4-bit unit)\n",
            "    conv1d_2_m_bias                : 32    (4-bit unit)\n",
            "    dense_weights                  : 192   (4-bit unit)\n",
            "    dense_bias                     : 6     (4-bit unit)\n",
            "\n",
            "Weight sparsity:\n",
            "... quantizing model\n",
            "    conv1d_0_m                     : 0.0357\n",
            "    conv1d_1_m                     : 0.0052\n",
            "    conv1d_2_m                     : 0.0026\n",
            "    dense                          : 0.3535\n",
            "    ----------------------------------------\n",
            "    Total Sparsity                 : 0.0075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umyv_Q97dd98"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2vsYhbK2tvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748e4983-23d0-4d3d-bd2f-0885fc751697"
      },
      "source": [
        "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
        "                                                 histogram_freq = 1,\n",
        "                                                 )\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 128, 9)]          0         \n",
            "_________________________________________________________________\n",
            "acti (QActivation)           (None, 128, 9)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_0_m (QConv1D)         (None, 126, 64)           1792      \n",
            "_________________________________________________________________\n",
            "act0_m (QActivation)         (None, 126, 64)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 63, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1_m (QConv1D)         (None, 61, 128)           24704     \n",
            "_________________________________________________________________\n",
            "act1_m (QActivation)         (None, 61, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 30, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2_m (QConv1D)         (None, 28, 32)            12320     \n",
            "_________________________________________________________________\n",
            "act2_m (QActivation)         (None, 28, 32)            0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "bn1 (QBatchNormalization)    (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "act3_m (QActivation)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (QDense)               (None, 6)                 198       \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 39,142\n",
            "Trainable params: 39,078\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWbQqribe1RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d8ee2c-214c-4375-88e3-d1d123671193"
      },
      "source": [
        "\n",
        "model_size=get_zipped_model_size(FILE)\n",
        "print(\" Size of qkeras cnn model: {:.2f}KB\".format(model_size/1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Size of qkeras cnn model: 445.75KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGwWy1sKP9q"
      },
      "source": [
        "#Post-Training Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYuQa9_oKPVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc16fa0f-ccfc-48cf-cde6-ee129d355177"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_file = '/content/drive/My Drive/Colab/MS Project/qkeras-master/qkeras-master/My Qkeras Model/qkeras_quantized_cnn_model.tflite'\n",
        "\n",
        "with open(tflite_file, 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp6j4v9ep2/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp6j4v9ep2/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeFalkMaKjxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5d366b-a6f9-4a20-d1c0-86524f1e3340"
      },
      "source": [
        "tflite_model_size=get_zipped_model_size(tflite_file)\n",
        "print(\" Size of quantized cnn model: {:.2f}KB\".format(tflite_model_size/1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Size of quantized cnn model: 32.61KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VluS-6GVKkeP"
      },
      "source": [
        "def evaluate_model(interpreter,testX,testy):\n",
        "  '''\n",
        "# Get input and output tensors.\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  print(input_details)\n",
        "  print(output_details)\n",
        "  print(\"== Input details ==\")\n",
        "  print(\"shape:\", input_details[0]['shape'])\n",
        "  print(\"type:\", input_details[0]['dtype'])\n",
        "  print(\"type:\", input_details[0]['index'])\n",
        "  print(\"\\n== Output details ==\")\n",
        "  print(\"shape:\", output_details[0]['shape'])\n",
        "  print(\"type:\", output_details[0]['dtype'])\n",
        "  print(\"type:\", output_details[0]['index'])\n",
        "\n",
        "\n",
        "\n",
        "  input_shape = input_details[0]['index']\n",
        "  output_shape = output_details[0]['index']\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction= []\n",
        "  print(testX.shape)\n",
        "  for i,test_data in enumerate(testX):\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "\n",
        "    test_data = np.expand_dims(test_data, axis=0).astype(np.float32)\n",
        "  \n",
        "    interpreter.set_tensor(input_shape, test_data)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.get_tensor(output_shape)\n",
        "    digit = np.argmax(output,axis=1)\n",
        "    prediction.append(digit)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  \n",
        "  print('\\n')\n",
        "  prediction=np.array(prediction)\n",
        "  print(prediction.shape)\n",
        "  print(testy.shape)\n",
        "  labels=np.argmax(testy,axis=-1)\n",
        "  accuracy_count=0\n",
        "  for i in range(len(prediction)):\n",
        "    \n",
        "    if(prediction[i]==labels[i]):\n",
        "      accuracy_count+=1\n",
        "  \n",
        "  accuracy=(accuracy_count/len(prediction))*100\n",
        "\n",
        "  return accuracy\n",
        "  '''\n",
        "  #Get input and output tensors.\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  print(input_details)\n",
        "  print(output_details)\n",
        "  print(\"== Input details ==\")\n",
        "  print(\"shape:\", input_details[0]['shape'])\n",
        "  print(\"type:\", input_details[0]['dtype'])\n",
        "  print(\"type:\", input_details[0]['index'])\n",
        "  print(\"\\n== Output details ==\")\n",
        "  print(\"shape:\", output_details[0]['shape'])\n",
        "  print(\"type:\", output_details[0]['dtype'])\n",
        "  print(\"type:\", output_details[0]['index'])\n",
        "\n",
        "  input_shape = input_details[0]['index']\n",
        "  output_shape = output_details[0]['index']\n",
        "\n",
        "  total_seen=0\n",
        "  correct=0\n",
        "\n",
        "  for data,label in zip(testX,testy):\n",
        "    data=np.expand_dims(data, axis=0).astype(np.float32)\n",
        "    total_seen+=1\n",
        "    interpreter.set_tensor(input_shape,data)\n",
        "    interpreter.invoke()\n",
        "    predictions=interpreter.get_tensor(output_shape)\n",
        "    if np.argmax(predictions)==np.argmax(label):\n",
        "      correct+=1\n",
        "    if total_seen%1000==0:\n",
        "      print(\"Accuracy after {:d} data : {:f}\".format(total_seen,float(correct)/float(total_seen)))\n",
        "\n",
        "  return float(correct)/float(total_seen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_m6FleXKol_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003f35f5-025c-4242-9a25-3957de6d38b9"
      },
      "source": [
        "\n",
        "interpreter=tf.lite.Interpreter(tflite_file)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "tflite_accuracy = evaluate_model(interpreter,testX,testy)\n",
        "\n",
        "print('Base TFLite test_accuracy:', tflite_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'name': 'input', 'index': 0, 'shape': array([  1, 128,   9], dtype=int32), 'shape_signature': array([ -1, 128,   9], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[{'name': 'Identity', 'index': 171, 'shape': array([1, 6], dtype=int32), 'shape_signature': array([-1,  6], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "== Input details ==\n",
            "shape: [  1 128   9]\n",
            "type: <class 'numpy.float32'>\n",
            "type: 0\n",
            "\n",
            "== Output details ==\n",
            "shape: [1 6]\n",
            "type: <class 'numpy.float32'>\n",
            "type: 171\n",
            "Accuracy after 1000 data : 0.832000\n",
            "Accuracy after 2000 data : 0.838500\n",
            "Base TFLite test_accuracy: 0.8734306073973532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTGL7HpdDLFh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}