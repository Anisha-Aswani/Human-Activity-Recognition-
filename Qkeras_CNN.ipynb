{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Qkeras CNN BN &PO2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1miUoB4oiUak"
      },
      "source": [
        "#Importing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtKs8iTj53r4"
      },
      "source": [
        "#Tensorflow version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkofaVEB30oN"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_KQZKwjzI2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a4346f-5bbe-4321-b182-96e06b726e07"
      },
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_model_optimization in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.6)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.19.5)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_aQv0xtgWMn",
        "outputId": "bcc92bb0-56f9-43b7-d53b-f42032ce6f02"
      },
      "source": [
        "!git clone https://github.com/google/qkeras.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'qkeras' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS7h-rFphEFl",
        "outputId": "154d9484-f51e-48da-84bc-c9f0b7146dc2"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/UCI HAR Dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-09 08:21:19--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60999314 (58M) [application/x-httpd-php]\n",
            "Saving to: ‘UCI HAR Dataset.zip.1’\n",
            "\n",
            "UCI HAR Dataset.zip 100%[===================>]  58.17M  31.2MB/s    in 1.9s    \n",
            "\n",
            "2021-06-09 08:21:21 (31.2 MB/s) - ‘UCI HAR Dataset.zip.1’ saved [60999314/60999314]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzNd__e7zBU8"
      },
      "source": [
        "import sys\n",
        "sys.path.append('qkeras')\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from qkeras import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_Hq0VUMvJx9"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from scipy import ndimage\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D,Conv2D\n",
        "from keras.layers.convolutional import MaxPooling1D,MaxPooling2D\n",
        "from keras.layers import BatchNormalization,ReLU,GlobalAveragePooling1D,MaxPooling1D,LSTM,TimeDistributed,GlobalAveragePooling2D\n",
        "#from keras.utils import to_categorical\n",
        "from keras.models import save_model, load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.utils import  plot_model\n",
        "from keras.models import Model,save_model,load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import concatenate\n",
        "import numpy as np\n",
        "from six.moves import zip\n",
        "from tensorflow.keras import callbacks\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import *\n",
        "\n",
        "from qkeras.utils import load_qmodel\n",
        "from qkeras.estimate import print_qstats\n",
        "from datetime import datetime\n",
        "\n",
        "from packaging import version\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import expand_dims\n",
        "from tensorflow import keras\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8tRrh2Q2gnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a939dce0-4bcc-42fa-d742-007fe52483af"
      },
      "source": [
        "!pip install -U tensorboard_plugin_profile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboard_plugin_profile\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/4e/0bf160776e5dacdba5105a580aa57a2bd37c1cc4faa1bd7695d82e7d6ae7/tensorboard_plugin_profile-2.4.0-py3-none-any.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 19.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 26.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 32.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 30.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 31.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 33.4MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 30.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 81kB 30.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 32.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 102kB 32.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112kB 32.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 122kB 32.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 133kB 32.7MB/s eta 0:00:01\r\u001b[K     |████                            | 143kB 32.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 153kB 32.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 163kB 32.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 174kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 194kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 204kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 215kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 225kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 235kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 245kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 256kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 266kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 276kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 286kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 296kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 307kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 317kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 327kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 337kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 348kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 358kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 368kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 378kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 389kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 399kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 409kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 419kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 430kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 440kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 450kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 460kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 471kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 481kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 491kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 501kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 512kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 522kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 532kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 542kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 552kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 563kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 573kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 583kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 593kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 604kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 614kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 624kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 634kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 645kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 655kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 665kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 675kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 686kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 696kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 706kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 716kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 727kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 737kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 747kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 757kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 768kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 778kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 788kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 798kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 808kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 819kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 829kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 839kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 849kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 860kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 870kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 880kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 890kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 901kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 911kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 921kB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 931kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 942kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 952kB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 962kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 972kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 983kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 993kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.0MB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.0MB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.0MB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0MB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.1MB 32.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.1MB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1MB 32.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1MB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1MB 32.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 32.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 32.7MB/s \n",
            "\u001b[?25hCollecting gviz-api>=1.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/8f/c6f16235a16b3dc4efdcf34dbc93b3b6f678b88176dbd6a36c75d678888f/gviz_api-1.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.15.0)\n",
            "Installing collected packages: gviz-api, tensorboard-plugin-profile\n",
            "Successfully installed gviz-api-1.9.0 tensorboard-plugin-profile-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INpMNpCdimZr"
      },
      "source": [
        "#Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vYcHaInEDoK"
      },
      "source": [
        "def load_file(filepath):\n",
        "  dataframe=read_csv(filepath,header=None,delim_whitespace=True)\n",
        "  return dataframe.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7akEplVIDyX"
      },
      "source": [
        "def load_group(filenames,prefix=''):\n",
        "  loaded=list()\n",
        "  for name in filenames:\n",
        "    data=load_file(prefix+name)\n",
        "    loaded.append(data)\n",
        "  loaded=dstack(loaded)\n",
        "  return loaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKHYErTsIYac"
      },
      "source": [
        "def load_dataset_group(group,prefix=''):\n",
        "  filepath=prefix+group+'/Inertial Signals/'\n",
        "  filenames=list()\n",
        "  filenames+=['total_acc_x_'+group+'.txt','total_acc_y_'+group+'.txt','total_acc_z_'+group+'.txt']\n",
        "  filenames+=['body_acc_x_'+group+'.txt','body_acc_y_'+group+'.txt','body_acc_z_'+group+'.txt']\n",
        "  filenames+=['body_gyro_x_'+group+'.txt','body_gyro_y_'+group+'.txt','body_gyro_z_'+group+'.txt']\n",
        "  X=load_group(filenames,filepath)\n",
        "  y=load_file(prefix+group+'/y_'+group+'.txt')\n",
        "  return (X,y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuWfdQHuJt9f"
      },
      "source": [
        "def load_dataset(prefix=''):\n",
        "  trainX,trainy=load_dataset_group('train',prefix+'UCI HAR Dataset/')\n",
        "  print(trainX.shape,trainy.shape)\n",
        "  testX,testy=load_dataset_group('test',prefix+'UCI HAR Dataset/')\n",
        "  print(testX.shape,testy.shape)\n",
        "  trainy=trainy-1\n",
        "  testy=testy-1\n",
        "  trainy=tf.compat.v1.keras.utils.to_categorical(trainy)\n",
        "  testy=tf.compat.v1.keras.utils.to_categorical(testy)\n",
        "  print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "  return trainX, trainy, testX, testy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Mtbf_PSH87"
      },
      "source": [
        "def scale_data(trainX, testX):\n",
        "\t# remove overlap\n",
        "\tcut = int(trainX.shape[1] / 2)\n",
        "\tlongX = trainX[:, -cut:, :]\n",
        "\t# flatten windows\n",
        "\tlongX = longX.reshape((longX.shape[0] * longX.shape[1], longX.shape[2]))\n",
        "\t# flatten train and test\n",
        "\tflatTrainX = trainX.reshape((trainX.shape[0] * trainX.shape[1], trainX.shape[2]))\n",
        "\tflatTestX = testX.reshape((testX.shape[0] * testX.shape[1], testX.shape[2]))\n",
        "\t# standardize\n",
        "\n",
        "\ts = StandardScaler()\n",
        "\t\t# fit on training data\n",
        "\ts.fit(longX)\n",
        "\t\t# apply to training and test data\n",
        "\tlongX = s.transform(longX)\n",
        "\tflatTrainX = s.transform(flatTrainX)\n",
        "\tflatTestX = s.transform(flatTestX)\n",
        "\t# reshape\n",
        "\tflatTrainX = flatTrainX.reshape((trainX.shape))\n",
        "\tflatTestX = flatTestX.reshape((testX.shape))\n",
        "\treturn flatTrainX, flatTestX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dow8qJO4myQ"
      },
      "source": [
        "##Function for model size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytB8g1p74mEU"
      },
      "source": [
        "def get_zipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  zipped_file = file+'.zip'\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JyEh2LQyggX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4415f418-f916-4d25-da16-503028b30a8f"
      },
      "source": [
        "optimizer=Adam(lr=0.0001)\n",
        "with_bn=1\n",
        "THRESHOLD=0.1\n",
        "\n",
        "class LearningRateAdjuster(callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    self.learning_rate_factor = 1.0\n",
        "    pass\n",
        "\n",
        "  def on_epoch_end(self, epochs, logs):\n",
        "    max_variance = -1\n",
        "\n",
        "    for layer in self.model.layers:\n",
        "      if layer.__class__.__name__ in [\n",
        "          \"BatchNormalization\",\n",
        "          \"QBatchNormalization\"\n",
        "      ]:\n",
        "        variance = np.max(layer.get_weights()[-1])\n",
        "        if variance > max_variance:\n",
        "          max_variance = variance\n",
        "\n",
        "    if max_variance > 32 and self.learning_rate_factor < 100:\n",
        "      learning_rate = K.get_value(self.model.optimizer.learning_rate)\n",
        "      self.learning_rate_factor /= 2.0\n",
        "      print(\"***** max_variance is {} / lr is {} *****\".format(\n",
        "          max_variance, learning_rate))\n",
        "      K.eval(K.update(\n",
        "          self.model.optimizer.learning_rate, learning_rate / 2.0\n",
        "      ))\n",
        "\n",
        "lra = LearningRateAdjuster()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOWuN0ZroUDR"
      },
      "source": [
        "##1. CNN MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7BkQ1pYraEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc8a0ce-dbd5-4376-c9db-b41ecfb6b599"
      },
      "source": [
        "trainX,trainy,testX,testy=load_dataset()\n",
        "verbose,epochs,batch_size=1,500,32 #500\n",
        "n_timesteps,n_features,n_outputs=trainX.shape[1],trainX.shape[2],trainy.shape[1]\n",
        "print('n step: ', n_timesteps)\n",
        "print('features: ',n_features)\n",
        "trainX,testX=scale_data(trainX,testX)\n",
        "print(trainX.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            "n step:  128\n",
            "features:  9\n",
            "(7352, 128, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yciMN9WVMkpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11475bf9-3105-451d-f190-02d8a990498f"
      },
      "source": [
        "\n",
        "'''\n",
        "\n",
        "inputs=keras.Input(shape=(n_timesteps,n_features))\n",
        "\n",
        "\n",
        "conv_1=tf.keras.layers.Conv1D(filters=64,kernel_size=5,strides=2,activation='relu')(inputs)\n",
        "maxpool_1=tf.keras.layers.MaxPooling1D(pool_size=2,strides=2)(conv_1)\n",
        "\n",
        "conv_2=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu')(maxpool_1)\n",
        "maxpool_2=tf.keras.layers.MaxPooling1D(pool_size=2,strides=1)(conv_2)\n",
        "\n",
        "conv_3=tf.keras.layers.Conv1D(filters=32,kernel_size=3,strides=1,activation='relu')(maxpool_2)\n",
        "avg_pooling=tf.keras.layers.GlobalAveragePooling1D()(conv_3)\n",
        "batch_norm=tf.keras.layers.BatchNormalization()(avg_pooling)\n",
        "\n",
        "output=tf.keras.layers.Dense(n_outputs,activation='softmax')(batch_norm)\n",
        "model=tf.keras.Model(inputs=inputs,outputs=output)\n",
        "'''\n",
        "\n",
        "x=x_in=Input(shape=(n_timesteps,n_features),name='input')\n",
        "x=QActivation(\"quantized_relu_po2(4,1)\",name=\"acti\")(x)\n",
        "x=QConv1D(filters=64, kernel_size=3,\n",
        "    strides=1,\n",
        "    kernel_quantizer=quantized_po2(4, 1),\n",
        "    bias_quantizer=quantized_bits(4,1) ,\n",
        "    name=\"conv1d_0_m\")(x)\n",
        "x = QActivation(\"quantized_relu(3,1)\", name=\"act0_m\")(x)\n",
        "x=tf.keras.layers.MaxPooling1D(pool_size=2,strides=2)(x)\n",
        "x=QConv1D(filters=128, kernel_size=3,\n",
        "    strides=1,\n",
        "    kernel_quantizer=quantized_po2(4, 1),\n",
        "    bias_quantizer=quantized_bits(4,1),\n",
        "    name=\"conv1d_1_m\")(x)\n",
        "x = QActivation(\"quantized_relu(3,1)\", name=\"act1_m\")(x)\n",
        "x=tf.keras.layers.MaxPooling1D(pool_size=2,strides=2)(x)\n",
        "x=QConv1D(filters=32, kernel_size=3,\n",
        "    strides=1,\n",
        "    kernel_quantizer=quantized_po2(4, 1),\n",
        "    bias_quantizer=quantized_bits(4,2,2),\n",
        "    name=\"conv1d_2_m\")(x)\n",
        "x = QActivation(\"quantized_relu(3,1)\", name=\"act2_m\")(x)\n",
        "x=tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "if with_bn:\n",
        "  x=QBatchNormalization(\n",
        "      gamma_quantizer=quantized_relu_po2(4,8),\n",
        "      variance_quantizer=quantized_relu_po2(6),\n",
        "      beta_quantizer=quantized_po2(4, 4),\n",
        "      gamma_range=8,\n",
        "      beta_range=4,\n",
        "      name=\"bn1\")(x)\n",
        "x = QActivation(\"quantized_relu(3,1)\", name=\"act3_m\")(x)\n",
        "x = Flatten()(x)\n",
        "x = QDense(\n",
        "    n_outputs,\n",
        "    kernel_quantizer=quantized_ulaw(4, 0, 1),\n",
        "    bias_quantizer=quantized_bits(4, 0, 1),\n",
        "    name=\"dense\")(\n",
        "        x)\n",
        "x = Activation(\"softmax\", name=\"softmax\")(x)\n",
        "model = Model(inputs=[x_in], outputs=[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "qkeras/qkeras/qnormalization.py:81: UserWarning: gamma_range is deprecated in QBatchNormalization layer.\n",
            "  warnings.warn('gamma_range is deprecated in QBatchNormalization layer.')\n",
            "qkeras/qkeras/qnormalization.py:84: UserWarning: beta_range is deprecated in QBatchNormalization layer.\n",
            "  warnings.warn('beta_range is deprecated in QBatchNormalization layer.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEENq8DHseVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23573ceb-1f07-4231-dbd4-536cd46a4107"
      },
      "source": [
        "plot_model(model, show_shapes=True, to_file='CNN_Model.png')\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "      trainX, trainy, batch_size=batch_size,\n",
        "      epochs=epochs, initial_epoch=1, verbose=1,\n",
        "      validation_split=0.1,\n",
        "      callbacks=[]) #lra])\n",
        "\n",
        "outputs = []\n",
        "output_names = []\n",
        "\n",
        "for layer in model.layers:\n",
        "  if layer.__class__.__name__ in [\n",
        "        \"QActivation\", \"QBatchNormalization\", \"Activation\", \"QDense\",\n",
        "        \"QConv1D\", \"QDepthwiseConv2D\"\n",
        "  ]:\n",
        "    output_names.append(layer.name)\n",
        "    outputs.append(layer.output)\n",
        "\n",
        "model_debug = Model(inputs=[x_in], outputs=outputs)\n",
        "\n",
        "outputs = model_debug.predict(trainX)\n",
        "\n",
        "print(\"{:30} {: 8.4f} {: 8.4f}\".format(\n",
        "      \"input\", np.min(trainX), np.max(trainX)))\n",
        "\n",
        "for n, p in zip(output_names, outputs):\n",
        "  print(\"{:30} {: 8.4f} {: 8.4f}\".format(n, np.min(p), np.max(p)), end=\"\")\n",
        "  layer = model.get_layer(n)\n",
        "  for i, weights in enumerate(layer.get_weights()):\n",
        "    if layer.get_quantizers()[i]:\n",
        "      weights = K.eval(layer.get_quantizers()[i](K.constant(weights)))\n",
        "    print(\" ({: 8.4f} {: 8.4f})\".format(np.min(weights), np.max(weights)),\n",
        "            end=\"\")\n",
        "  print(\"\")\n",
        "\n",
        "score = model.evaluate(testX, testy, verbose=False)\n",
        "print(\"Test score:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "FILE='model.h5'\n",
        "model.save(FILE)\n",
        "print_qstats(model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/5\n",
            "207/207 [==============================] - 36s 12ms/step - loss: 1.4003 - accuracy: 0.5401 - val_loss: 0.9067 - val_accuracy: 0.7595\n",
            "Epoch 3/5\n",
            "207/207 [==============================] - 2s 8ms/step - loss: 0.7201 - accuracy: 0.7502 - val_loss: 0.5576 - val_accuracy: 0.8818\n",
            "Epoch 4/5\n",
            "207/207 [==============================] - 2s 8ms/step - loss: 0.5361 - accuracy: 0.8425 - val_loss: 0.4428 - val_accuracy: 0.8872\n",
            "Epoch 5/5\n",
            "207/207 [==============================] - 2s 7ms/step - loss: 0.4422 - accuracy: 0.8753 - val_loss: 0.2878 - val_accuracy: 0.9497\n",
            "input                          -15.7022  15.1091\n",
            "acti                             0.0000   1.0000\n",
            "conv1d_0_m                      -3.7109   3.8437 ( -0.5000   0.5000) (  0.0000   0.0000)\n",
            "act0_m                           0.0000   1.7500\n",
            "conv1d_1_m                      -5.4844   4.9189 ( -0.2500   0.2500) (  0.0000   0.0000)\n",
            "act1_m                           0.0000   1.7500\n",
            "conv1d_2_m                      -4.9561   6.7461 ( -0.2500   0.2500) (  0.0000   0.0000)\n",
            "act2_m                           0.0000   1.7500\n",
            "bn1                             -5.5719   8.7829 (  1.0000   1.0000) ( -0.0625   0.0625) (  0.0039   2.0000) (  0.0002   0.5000)\n",
            "act3_m                           0.0000   1.7500\n",
            "dense                           -6.9152   8.7610 ( -0.8750   0.8750) (  0.0000   0.0000)\n",
            "softmax                          0.0000   0.9993\n",
            "Test score: 0.4727879762649536\n",
            "Test accuracy: 0.8500169515609741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "qkeras/qkeras/qnormalization.py:81: UserWarning: gamma_range is deprecated in QBatchNormalization layer.\n",
            "  warnings.warn('gamma_range is deprecated in QBatchNormalization layer.')\n",
            "qkeras/qkeras/qnormalization.py:84: UserWarning: beta_range is deprecated in QBatchNormalization layer.\n",
            "  warnings.warn('beta_range is deprecated in QBatchNormalization layer.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From qkeras/qkeras/estimate.py:345: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From qkeras/qkeras/estimate.py:345: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of operations in model:\n",
            "    conv1d_0_m                    : 217728 (sadder_4_4)\n",
            "    conv1d_1_m                    : 1499136 (sbarrel_4_3)\n",
            "    conv1d_2_m                    : 344064 (sbarrel_4_3)\n",
            "    dense                         : 192   (smult_4_3)\n",
            "\n",
            "Number of operation types in model:\n",
            "    sadder_4_4                    : 217728\n",
            "    sbarrel_4_3                   : 1843200\n",
            "    smult_4_3                     : 192\n",
            "\n",
            "Weight profiling:\n",
            "    conv1d_0_m_weights             : 1728  (4-bit unit)\n",
            "    conv1d_0_m_bias                : 64    (4-bit unit)\n",
            "    conv1d_1_m_weights             : 24576 (4-bit unit)\n",
            "    conv1d_1_m_bias                : 128   (4-bit unit)\n",
            "    conv1d_2_m_weights             : 12288 (4-bit unit)\n",
            "    conv1d_2_m_bias                : 32    (4-bit unit)\n",
            "    dense_weights                  : 192   (4-bit unit)\n",
            "    dense_bias                     : 6     (4-bit unit)\n",
            "\n",
            "Weight sparsity:\n",
            "... quantizing model\n",
            "    conv1d_0_m                     : 0.0357\n",
            "    conv1d_1_m                     : 0.0052\n",
            "    conv1d_2_m                     : 0.0026\n",
            "    dense                          : 0.3889\n",
            "    ----------------------------------------\n",
            "    Total Sparsity                 : 0.0077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umyv_Q97dd98"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2vsYhbK2tvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1454ac73-2e71-4164-c7ae-89a17736c10e"
      },
      "source": [
        "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
        "                                                 histogram_freq = 1,\n",
        "                                                 )\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 128, 9)]          0         \n",
            "_________________________________________________________________\n",
            "acti (QActivation)           (None, 128, 9)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_0_m (QConv1D)         (None, 126, 64)           1792      \n",
            "_________________________________________________________________\n",
            "act0_m (QActivation)         (None, 126, 64)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 63, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1_m (QConv1D)         (None, 61, 128)           24704     \n",
            "_________________________________________________________________\n",
            "act1_m (QActivation)         (None, 61, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 30, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2_m (QConv1D)         (None, 28, 32)            12320     \n",
            "_________________________________________________________________\n",
            "act2_m (QActivation)         (None, 28, 32)            0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "bn1 (QBatchNormalization)    (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "act3_m (QActivation)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (QDense)               (None, 6)                 198       \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 39,142\n",
            "Trainable params: 39,078\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWbQqribe1RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbfb2e1-9fb0-4b1d-e8f6-5b2bcdb1bb90"
      },
      "source": [
        "\n",
        "model_size=get_zipped_model_size(FILE)\n",
        "print(\" Size of qkeras cnn model: {:.2f}KB\".format(model_size/1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Size of qkeras cnn model: 445.42KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGwWy1sKP9q"
      },
      "source": [
        "#Post-Training Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYuQa9_oKPVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa72e3c-060f-40e9-bb48-9101f7f989d6"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_file = 'qkeras_quantized_cnn_model.tflite'\n",
        "\n",
        "with open(tflite_file, 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpmifhheth/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpmifhheth/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeFalkMaKjxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30b91f2-5d86-405c-c5e3-48487a094dda"
      },
      "source": [
        "tflite_model_size=get_zipped_model_size(tflite_file)\n",
        "print(\" Size of quantized cnn model: {:.2f}KB\".format(tflite_model_size/1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Size of quantized cnn model: 32.38KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VluS-6GVKkeP"
      },
      "source": [
        "def evaluate_model(interpreter,testX,testy):\n",
        "  '''\n",
        "# Get input and output tensors.\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  print(input_details)\n",
        "  print(output_details)\n",
        "  print(\"== Input details ==\")\n",
        "  print(\"shape:\", input_details[0]['shape'])\n",
        "  print(\"type:\", input_details[0]['dtype'])\n",
        "  print(\"type:\", input_details[0]['index'])\n",
        "  print(\"\\n== Output details ==\")\n",
        "  print(\"shape:\", output_details[0]['shape'])\n",
        "  print(\"type:\", output_details[0]['dtype'])\n",
        "  print(\"type:\", output_details[0]['index'])\n",
        "\n",
        "\n",
        "\n",
        "  input_shape = input_details[0]['index']\n",
        "  output_shape = output_details[0]['index']\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction= []\n",
        "  print(testX.shape)\n",
        "  for i,test_data in enumerate(testX):\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "\n",
        "    test_data = np.expand_dims(test_data, axis=0).astype(np.float32)\n",
        "  \n",
        "    interpreter.set_tensor(input_shape, test_data)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.get_tensor(output_shape)\n",
        "    digit = np.argmax(output,axis=1)\n",
        "    prediction.append(digit)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  \n",
        "  print('\\n')\n",
        "  prediction=np.array(prediction)\n",
        "  print(prediction.shape)\n",
        "  print(testy.shape)\n",
        "  labels=np.argmax(testy,axis=-1)\n",
        "  accuracy_count=0\n",
        "  for i in range(len(prediction)):\n",
        "    \n",
        "    if(prediction[i]==labels[i]):\n",
        "      accuracy_count+=1\n",
        "  \n",
        "  accuracy=(accuracy_count/len(prediction))*100\n",
        "\n",
        "  return accuracy\n",
        "  '''\n",
        "  #Get input and output tensors.\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  print(input_details)\n",
        "  print(output_details)\n",
        "  print(\"== Input details ==\")\n",
        "  print(\"shape:\", input_details[0]['shape'])\n",
        "  print(\"type:\", input_details[0]['dtype'])\n",
        "  print(\"type:\", input_details[0]['index'])\n",
        "  print(\"\\n== Output details ==\")\n",
        "  print(\"shape:\", output_details[0]['shape'])\n",
        "  print(\"type:\", output_details[0]['dtype'])\n",
        "  print(\"type:\", output_details[0]['index'])\n",
        "\n",
        "  input_shape = input_details[0]['index']\n",
        "  output_shape = output_details[0]['index']\n",
        "\n",
        "  total_seen=0\n",
        "  correct=0\n",
        "\n",
        "  for data,label in zip(testX,testy):\n",
        "    data=np.expand_dims(data, axis=0).astype(np.float32)\n",
        "    total_seen+=1\n",
        "    interpreter.set_tensor(input_shape,data)\n",
        "    interpreter.invoke()\n",
        "    predictions=interpreter.get_tensor(output_shape)\n",
        "    if np.argmax(predictions)==np.argmax(label):\n",
        "      correct+=1\n",
        "    if total_seen%1000==0:\n",
        "      print(\"Accuracy after {:d} data : {:f}\".format(total_seen,float(correct)/float(total_seen)))\n",
        "\n",
        "  return float(correct)/float(total_seen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_m6FleXKol_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c1deaf-87d7-4007-ac8f-5d426690986e"
      },
      "source": [
        "\n",
        "interpreter=tf.lite.Interpreter(tflite_file)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "tflite_accuracy = evaluate_model(interpreter,testX,testy)\n",
        "\n",
        "print('Base TFLite test_accuracy:', tflite_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'name': 'input', 'index': 0, 'shape': array([  1, 128,   9], dtype=int32), 'shape_signature': array([ -1, 128,   9], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[{'name': 'Identity', 'index': 170, 'shape': array([1, 6], dtype=int32), 'shape_signature': array([-1,  6], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "== Input details ==\n",
            "shape: [  1 128   9]\n",
            "type: <class 'numpy.float32'>\n",
            "type: 0\n",
            "\n",
            "== Output details ==\n",
            "shape: [1 6]\n",
            "type: <class 'numpy.float32'>\n",
            "type: 170\n",
            "Accuracy after 1000 data : 0.802000\n",
            "Accuracy after 2000 data : 0.807500\n",
            "Base TFLite test_accuracy: 0.8523922633186292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTGL7HpdDLFh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}